{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston housing dataset has 489 data points with 4 variables each.\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries necessary for this project\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Import supplementary visualizations code visuals.py\n",
    "import visuals as vs\n",
    "\n",
    "#Data file - 'housing.csv'\n",
    "in_data = \"housing.csv\"\n",
    "\n",
    "#Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "#Load the Boston housing dataset\n",
    "data = pd.read_csv(in_data)\n",
    "\n",
    "#Display first 5 data enteries to check everything works fine till now\n",
    "data.head()\n",
    "\n",
    "#Split the data into features and prices\n",
    "features = data.drop('MEDV', axis = 1)\n",
    "prices = data['MEDV']\n",
    "\n",
    "#Success!\n",
    "print(\"Boston housing dataset has {} data points with {} variables each.\".format(*data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum price in the data is -> $105000.0\n",
      "The maximum price in the data is -> $1024800.0\n",
      "The mean price of the data is -> $454342.9447852761\n",
      "The median price of the data is -> $438900.0\n",
      "The standard deviation of prices of the data is -> $165171.13154429477\n"
     ]
    }
   ],
   "source": [
    "#Calculating descriptive statistics about Boston Housing prices\n",
    "\n",
    "#Minimum price of the data\n",
    "minimum_price = np.amin(prices)\n",
    "print(\"The minimum price in the data is -> ${}\".format(minimum_price))\n",
    "\n",
    "#Maximum price of the data\n",
    "maximum_price = np.amax(prices)\n",
    "print(\"The maximum price in the data is -> ${}\".format(maximum_price))\n",
    "\n",
    "#Mean price of the data\n",
    "mean_price = np.mean(prices)\n",
    "print(\"The mean price of the data is -> ${}\".format(mean_price))\n",
    "\n",
    "#Median price of the data\n",
    "median_price = np.median(prices)\n",
    "print(\"The median price of the data is -> ${}\".format(median_price))\n",
    "\n",
    "#Standard deviation of prices of the data\n",
    "std_price = np.std(prices)\n",
    "print(\"The standard deviation of prices of the data is -> ${}\".format(std_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: **\n",
    "\n",
    "* If the 'RM' value (Average number of rooms in the houses in the neighbourhood) increases then 'MEDV' of a house also increases and vice versa. This is because, We can conclude that a house with a higher 'RM' value has an elite neibhourhood (as the neighboring houses are larger, have more rooms), therefore neighboring homes are generally expensive. And houses located in an expensive neighborhood tends to be more expensive than houses with lesser 'RM' values.\n",
    "\n",
    "\n",
    "* If the 'LSTAT' value (Percent of lower class workers living in the neighbourhood) increases then 'MEDV' of a house decreases and vice versa. This can be explained as follows : If a house has higher 'LSTAT' value then it means that is surrounded by lower class people, and therefore the neighborhood is poor. And generally prices of the houses in a poor neighborhood tends to be lesser than the prices of houses in an well-off neighborhood.\n",
    "\n",
    "\n",
    "* If the 'PTRATIO' value (Ratio of students to teachers living in the neighborhood) increases then 'MEDV' value of a house decreases and vice-versa. This is because, we can conclude that a house with higher 'PTRATIO' value has more number of children in the neighboring houses than number of teachers. Such a neighborhood is not generally quite, because of the presence of these little troublemakers. The children may harm other homes while playing (for e.g. : Break windows, ruin the perfect lawns, etc, of other house). In other words, The neighborhood is not very quite and peaceful. Therefore the prices of the houses in the following neighborhood are lesser as compared to other houses where 'PTRATIO' is more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: **\n",
    "A model trained with a good and optimal split of training and testing data tends to work well as compared to other models. If the dataset used for training the model is optimal, then the model will perform well on the testing set.\n",
    "This is the reason a model must be trained and tested with good splits of data.\n",
    "This can be explained as follows:\n",
    "\n",
    "* If the training dataset is insufficient for recognizing and generalizing the whole dataset, then the model trained with such a dataset will fail to capture the variation and complexity of the data in the dataset. And will perform bad on the testing set. This is termed as underfitting and the model is called 'High Bias Model'. In other words this is over-simplifying the problem.\n",
    "\n",
    "* If the training dataset is sufficient and optimal then the model trained with such a dataset will have better integrity in generalizing the dataset, and will capture the variation and complexity of the dataset with good precision. Such a model will perform well on training and testing dataset. This is the perfect model.\n",
    "\n",
    "* If a model is overtrained with a training dataset then the model tends to overgeneralise and hence universalize the dataset it is trained with. In such a case, the model will perform flawlessly on the training dataset as it tends to remember the data-points instead of predicting them. But it will perform poorly on the testing dataset because it will fail to predict any new data (other than the data it was initially trained with). This is termed as overfitting and the model is called 'High Variance Model'. In other words this is over-complicating the problem.\n",
    "\n",
    "Therefore, We can conclude that a good splitting of dataset into training and testing subsets is important for a learning algorithm to work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: **\n",
    "* The maximum depth parameter for the model with best learning performance is 3 (max_depth = 3) as training and testing curves are converging at a higher score than the other three models.\n",
    "\n",
    "* The score of the training curve decreases as more training points are added whereas the score of the testing curve   increases as more training points are added. But after a certain point, increment in training points will not substantially effect the scores of training and testing curves. The scores would slightly change with the increase in training points. The scores will approximately remain constant and the curves tend to converge at a certain point.\n",
    "\n",
    "* If the training and testing curves are converging with a score above the benchmark threshold, then adding more training points will not benefit the model. As after a certain point, the scores (of training and testing curves) do not change substantially with any increment in training points. Rather, the scores tend to converge at a single point and approximately remain constant. Therefore, adding more training points in this situation will only increase time complexity of the learning algorithm, hence we conclude that its not beneficial for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: **\n",
    "\n",
    "* When the model is trained with a maximum depth of 1, then it suffers from High bias which is a sign of underfitting. The model is not complex enough to generalize the data and, capture the complexity and variations in the data. In other words this is over-simplifying the problem.\n",
    "\n",
    "* When the model is trained with a maximum depth of 10, then it suffers from High variance or overfitting. In this case, the model performs flawlessly on the training dataset as it tends to remember the data-points instead of predicting them. But, performs poorly on the cross-validation dataset because it fails to predict any new data (other than the data it was initially trained with). The model is memorising the data and cannot generalize well. In other words this is over-complicating the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: **\n",
    "The model with 'maximum depth = 4' is the best, and offers best performance in generalising the unseen data. This is because:\n",
    "\n",
    "* The score for cross-validation curve is maximum at maximum depth = 4.\n",
    "* The curves (training and cross-validation) tends to converge at this point.\n",
    "* The curves (training and cross-validation) are diverging when maximum depth parameter is further increased (max_depth > 4), which implies that the model tends to overfit the data and hence become high variance model.\n",
    "* For maximum depth > 4, Training score is increasing while cross-validation score is decreasing. Which is a sign of overfitting, and hence high variance in model.\n",
    "* If maximum depth < 4, then the curves (training and cross-validation) are scoring less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
